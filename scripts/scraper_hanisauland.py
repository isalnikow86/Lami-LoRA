import requests
from bs4 import BeautifulSoup
import json
import time
import string

BASE_URL = "https://www.hanisauland.de"
LEXIKON_BASE = "/wissen/lexikon/grosses-lexikon/"

OUTPUT_FILE = "data/oer_texts_hanisauland.jsonl"
LETTERS = list(string.ascii_lowercase)

def get_links(letter):
    url = BASE_URL + LEXIKON_BASE + letter + "/"
    print(f"üîç Lade {url}")
    try:
        res = requests.get(url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        # Richtiges Element f√ºr Artikel-Links
        links = [a["href"] for a in soup.select("a.dictionary-word") if a.has_attr("href")]
        print(f"üîó {len(links)} Links gefunden")
        return links
    except Exception as e:
        print(f"‚ö† Fehler beim Laden der Seite {url}: {e}")
        return []


def scrape_article(path):
    full_url = BASE_URL + path if path.startswith("/") else path
    try:
        res = requests.get(full_url, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        content = soup.select_one("div.text")  # Inhalt des Lexikon-Artikels
        if not content:
            print(f"‚ö† Kein Content gefunden: {full_url}")
            return None

        title = soup.title.text.strip() if soup.title else "Kein Titel"
        text = content.get_text(separator="\n").strip()
        if len(text) > 200:
            return {"url": full_url, "title": title, "text": text}
        else:
            print(f"‚ö† Artikel zu kurz: {full_url}")
    except Exception as e:
        print(f"‚ö† Fehler bei Artikel {path}: {e}")
    return None

if __name__ == "__main__":
    print("‚ñ∂ Starte Scraping hanisauland.de (Gro√ües Lexikon)...")
    all_articles = []

    for letter in string.ascii_lowercase:
    print(f"üî° Buchstabe {letter.upper()}...", end=" ")
    links = get_links(letter)  # ‚Üê hier korrigiert!
    print(f"{len(links)} Links gefunden")

    for link in links:
        data = scrape_article(link)
        if data:
            all_articles.append(data)
            print(f"‚úÖ {data['title']}")
        time.sleep(0.3)


    with open(OUTPUT_FILE, "w") as f:
        for item in all_articles:
            json.dump(item, f, ensure_ascii=False)
            f.write("\n")

    print(f"\n‚úÖ Scraping abgeschlossen. {len(all_articles)} Artikel gespeichert ‚Üí {OUTPUT_FILE}")
