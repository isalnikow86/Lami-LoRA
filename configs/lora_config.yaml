base_model: LeoLM/leo-hessianai-7b
output_dir: lora-outputs/
learning_rate: 5e-5
batch_size: 8
num_train_epochs: 3
logging_steps: 10
save_steps: 100
lr_scheduler_type: cosine
warmup_steps: 100
weight_decay: 0.01
